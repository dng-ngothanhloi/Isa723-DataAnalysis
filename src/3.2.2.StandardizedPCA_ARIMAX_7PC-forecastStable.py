#3.2.2.StandardizedPCA_ARIMAX_7PC-forecastStable.py
# from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import RobustScaler
from sklearn.decomposition import PCA
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.statespace.sarimax import SARIMAX
from scipy.stats import skew, kurtosis
from tabulate import tabulate
import warnings
from google.colab import files  # Import files for download
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import itertools

warnings.filterwarnings("ignore")

# 1.1 ƒê·ªçc d·ªØ li·ªáu & Chu·∫©n ho√° - step c·ªßa ph·∫ßn tr∆∞·ªõc
# Use a different variable name for the initial read to avoid confusion with the standardized df
df = pd.read_csv("VNINDEX_iqr_filled_HSG_Adjust.csv", index_col="Day", parse_dates=True)
# Chu·∫©n h√≥a z-score (bao g·ªìm mean centering)
# ƒê·ªçc d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a
##===== C·∫£i ti·∫øn c√¥ng ƒëo·∫°n chu·∫©n ho√° v·ªõi StandardScaler ==============
# scaler = StandardScaler()
# df_standardized = pd.DataFrame(
#    scaler.fit_transform(df), columns=df.columns, index=df.index
# )
# print("ƒê√£ l∆∞u d·ªØ li·ªáu chu·∫©n h√≥a v√†o VNINDEX_standardized.csv")
# 1.2 Ki·ªÉm tra trung b√¨nh v√† ƒë·ªô l·ªách chu·∫©n => ƒê√£ x·ª≠ l√Ω b∆∞·ªõc chu·∫©n ho√° d·ªØ li·ªáu
# means = df_standardized.mean()
# stds = df_standardized.std()
## ƒê√£ c√≥ b√°o c√°o ri√™ng-ch∆∞a c·∫ßn in
# print("Trung b√¨nh c·ªßa d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a:")
# print(means)
# print("\nƒê·ªô l·ªách chu·∫©n c·ªßa d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a:")
# print(stds)
# df_standardized.to_csv('VNINDEX_standardized.csv')
# try:
#    files.download('VNINDEX_standardized.csv')
#    print("ƒê√£ l∆∞u d·ªØ li·ªáu chu·∫©n h√≥a v√†o VNINDEX_standardized.csv")
# except NameError:
#    print("Ch·∫°y trong m√¥i tr∆∞·ªùng kh√¥ng ph·∫£i Colab, b·ªè qua download file.")
###------- thay th·∫ø b·∫±ng RobustScaler v√† xo√° d·ªØ li·ªáu tr√πng ----
# Lo·∫°i b·ªè ng√†y tr√πng l·∫∑p (ƒë·ªÅ xu·∫•t th√™m ƒë·ªÉ c·∫£i thi·ªán d·ªØ li·ªáu)
print(f"S·ªë l∆∞·ª£ng ng√†y tr∆∞·ªõc Orginal c·ªßa d·ªØ li·ªáu: {len(df)}")
df = df.drop_duplicates()
print(f"S·ªë l∆∞·ª£ng ng√†y sau khi lo·∫°i b·ªè tr√πng l·∫∑p: {len(df)}")
# Chu·∫©n h√≥a b·∫±ng RobustScaler
scaler = RobustScaler()
df_standardized = pd.DataFrame(scaler.fit_transform(df), columns=df.columns, index=df.index)
# 1.2 Ki·ªÉm tra trung v·ªã v√† IQR
print("Trung v·ªã trung b√¨nh c·ªßa d·ªØ li·ªáu:", df_standardized.median().mean())
print("IQR trung b√¨nh c·ªßa d·ªØ li·ªáu:", (df_standardized.quantile(0.75) - df_standardized.quantile(0.25)).mean())
means = df_standardized.mean()
stds = df_standardized.std()
## ƒê√£ c√≥ b√°o c√°o ri√™ng-ch∆∞a c·∫ßn in
print("Trung b√¨nh c·ªßa d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a:")
print(means)
print("\nƒê·ªô l·ªách chu·∫©n c·ªßa d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a:")
print(stds)

df_standardized.to_csv("VNINDEX_robust_standardized.csv")
try:
    files.download("VNINDEX_robust_standardized.csv")
    print("ƒê√£ l∆∞u d·ªØ li·ªáu chu·∫©n h√≥a v√†o VNINDEX_robust_standardized.csv")
except NameError:
    print("Ch·∫°y trong m√¥i tr∆∞·ªùng kh√¥ng ph·∫£i Colab, b·ªè qua download file.")

print("ƒê√£ l∆∞u d·ªØ li·ªáu chu·∫©n h√≥a v√†o VNINDEX_robust_standardized.csv")
##=============== END


# 2 T√≠nh to√°n ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai v√† ma tr·∫≠n t∆∞∆°ng quan Pearson
##2.1 T√°ch bi·∫øn m·ª•c ti√™u tr∆∞·ªõc khi t√≠nh ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai v√† t∆∞∆°ng quan
## Vai tr√≤ c·ªßa VNINDEX trong ph√¢n t√≠ch & L√Ω do lo·∫°i b·ªè variant m·ª•c ti√™u
##  VNINDEX l√† bi·∫øn m·ª•c ti√™u m√† b·∫°n mu·ªën d·ª± b√°o b·∫±ng m√¥ h√¨nh ARIMAX. Trong b√†i to√°n n√†y, m·ª•c ti√™u l√† s·ª≠ d·ª•ng c√°c bi·∫øn ƒë·∫ßu v√†o (features) ƒë·ªÉ gi·∫£i th√≠ch v√† d·ª± ƒëo√°n gi√° tr·ªã c·ªßa VNINDEX.
##  Khi x√¢y d·ª±ng m√¥ h√¨nh nh∆∞ ARIMAX, bi·∫øn m·ª•c ti√™u th∆∞·ªùng ƒë∆∞·ª£c t√°ch ra kh·ªèi t·∫≠p d·ªØ li·ªáu ƒë·∫ßu v√†o ƒë·ªÉ tr√°nh vi·ªác c√°c bi·∫øn ƒë·ªôc l·∫≠p b·ªã ·∫£nh h∆∞·ªüng b·ªüi ch√≠nh bi·∫øn c·∫ßn d·ª± ƒëo√°n, d·∫´n ƒë·∫øn sai l·ªách trong ph√¢n t√≠ch.
##  N·∫øu b·∫°n ƒë·ªÉ VNINDEX trong X khi ch·∫°y PCA, n√≥ s·∫Ω b·ªã coi l√† m·ªôt bi·∫øn ƒë·∫ßu v√†o, ƒëi·ªÅu n√†y kh√¥ng ƒë√∫ng v·ªõi m·ª•c ti√™u ph√¢n t√≠ch (v√¨ VNINDEX l√† bi·∫øn c·∫ßn d·ª± ƒëo√°n, kh√¥ng ph·∫£i bi·∫øn gi·∫£i th√≠ch).
## L·ª£i √≠ch lo·∫°i b·ªè VNIndex:
##  T·∫≠p trung v√†o bi·∫øn ƒë·ªôc l·∫≠p: B·∫±ng c√°ch lo·∫°i b·ªè VNINDEX, ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai v√† ma tr·∫≠n t∆∞∆°ng quan ch·ªâ ph·∫£n √°nh m·ªëi quan h·ªá gi·ªØa c√°c bi·∫øn ƒë·∫ßu v√†o, gi√∫p PCA x√°c ƒë·ªãnh c√°c th√†nh ph·∫ßn ch√≠nh d·ª±a tr√™n s·ª± bi·∫øn thi√™n c·ªßa c√°c y·∫øu t·ªë ƒë·ªôc l·∫≠p.
##  Tr√°nh r√≤ r·ªâ d·ªØ li·ªáu (data leakage): N·∫øu VNINDEX ƒë∆∞·ª£c gi·ªØ l·∫°i trong qu√° tr√¨nh PCA, th√¥ng tin t·ª´ bi·∫øn m·ª•c ti√™u c√≥ th·ªÉ "r√≤ r·ªâ" v√†o c√°c th√†nh ph·∫ßn ch√≠nh, l√†m gi·∫£m t√≠nh kh√°ch quan v√† hi·ªáu qu·∫£ c·ªßa m√¥ h√¨nh ARIMAX khi d·ª± b√°o.
##  Chu·∫©n b·ªã cho ARIMAX: ARIMAX y√™u c·∫ßu c√°c bi·∫øn ngo·∫°i sinh (exogenous variables) l√† c√°c y·∫øu t·ªë ƒë·ªôc l·∫≠p v·ªõi bi·∫øn m·ª•c ti√™u. C√°c th√†nh ph·∫ßn ch√≠nh t·ª´ PCA tr√™n t·∫≠p X (sau khi lo·∫°i b·ªè VNINDEX) s·∫Ω ƒë√≥ng vai tr√≤ l√† bi·∫øn ngo·∫°i sinh ph√π h·ª£p.
if "VNINDEX" in df_standardized.columns:
    X_Variant = df_standardized.drop(columns=["VNINDEX"])
    y_Target = df_standardized["VNINDEX"]  # Keep target for later
else:
    print("Warning: 'VNINDEX' column not found in the standardized DataFrame.")
    raise ValueError("'VNINDEX' column not found after standardization. Cannot proceed.")

# 2.2 T√≠nh Ma tr·∫≠n Hi·ªáp ph∆∞∆°ng sai t·ª´ d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a
# Ph∆∞∆°ng sai (variance) c·ªßa m·ªôt bi·∫øn ƒë√£ chu·∫©n h√≥a l√† 1
# Hi·ªáp ph∆∞∆°ng sai (covariance) gi·ªØa hai bi·∫øn ƒë√£ chu·∫©n h√≥a l√† t∆∞∆°ng quan c·ªßa ch√∫ng
covariance_matrix = X_Variant.cov()

# 2.3 T√≠nh Ma tr·∫≠n T∆∞∆°ng quan (l√Ω thuy·∫øt l√† gi·ªëng ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai c·ªßa data chu·∫©n h√≥a)
correlation_matrix = X_Variant.corr()
## T·∫°m th·ªùi ch∆∞a c√¢n in ma tr√¢n
##print("Ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai (5x5 ƒë·∫ßu ti√™n):")
##print(covariance_matrix.iloc[:5, :5])
##print("\nMa tr·∫≠n t∆∞∆°ng quan Pearson (5x5 ƒë·∫ßu ti√™n):")
##print(correlation_matrix.iloc[:5, :5])
##print("\nMa tr·∫≠n t∆∞∆°ng quan:\n", correlation_matrix)

# So s√°nh hai ma tr·∫≠n b·∫±ng np.allclose
# allclose ki·ªÉm tra xem hai m·∫£ng c√≥ g·∫ßn gi·ªëng nhau trong dung sai cho ph√©p hay kh√¥ng
# rtol (relative tolerance): dung sai t∆∞∆°ng ƒë·ªëi
# atol (absolute tolerance): dung sai tuy·ªát ƒë·ªëi
are_matrices_close = np.allclose(covariance_matrix, correlation_matrix, atol=1e-5)

print(
    f"Ma tr·∫≠n Hi·ªáp ph∆∞∆°ng sai v√† Ma tr·∫≠n T∆∞∆°ng quan c√≥ g·∫ßn gi·ªëng nhau kh√¥ng (s·ª≠ d·ª•ng np.allclose)? {are_matrices_close}"
)

# . L∆∞u d·ªØ li·ªáu chu·∫©n h√≥a v√† ma tr·∫≠n t∆∞∆°ng quan
correlation_matrix.to_csv("VNINDEX_correlation_matrix.csv")
covariance_matrix.to_csv("VNINDEX_covariance_matrix.csv")
try:
    files.download("VNINDEX_correlation_matrix.csv")
    files.download("VNINDEX_covariance_matrix.csv")
    print("\nƒê√£ l∆∞u ma tr·∫≠n t∆∞∆°ng quan v√†o VNINDEX_correlation_matrix.csv")
    print("ƒê√£ l∆∞u ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai VNINDEX_covariance_matrix.csv")
except NameError:
    print("\nRunning in a non-Colab environment, skipping file downloads.")
    print("Files saved locally: VNINDEX_correlation_matrix.csv, VNINDEX_covariance_matrix.csv")

print("\nMa tr·∫≠n T∆∞∆°ng quan Pearson:\n")
# S·ª≠ d·ª•ng tabulate ƒë·ªÉ hi·ªÉn th·ªã ma tr·∫≠n ƒë·∫πp h∆°n trong output console
# ƒê√£ ƒë·∫°t - h·∫°n ch·∫ø print Console
# from tabulate import tabulate
# print(tabulate(correlation_matrix, headers='keys', tablefmt='grid', floatfmt=".4f"))

# 2.4 Tr·ª±c quan h√≥a Ma tr·∫≠n T∆∞∆°ng quan b·∫±ng Heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Heatmap Ma tr·∫≠n T∆∞∆°ng quan Pearson (Kh√¥ng bao g·ªìm VNINDEX)")
plt.tight_layout()
plt.savefig("Correlation_heatmap.png")
plt.show()
plt.close()
try:
    files.download("Correlation_heatmap.png")
    print("\nƒê√£ l∆∞u heatmap ma tr·∫≠n t∆∞∆°ng quan v√†o 'correlation_heatmap.png'")
except NameError:
    print("\nRunning in a non-Colab environment, skipping file downloads.")
    print("Heatmap saved locally: Correlation_heatmap.png")


# 2.5. ƒê√°nh gi√° M·ªëi quan h·ªá T∆∞∆°ng quan (tr√™n X_Variant)
# print("\n--- ƒê√°nh gi√° Chi ti·∫øt M·ªëi quan h·ªá T∆∞∆°ng quan ---")

# 2.5.1 Ki·ªÉm tra/t√¨m c√°c c·∫∑p bi·∫øn c√≥ t∆∞∆°ng quan cao (v√≠ d·ª•: > 0.8 ho·∫∑c < -0.8, lo·∫°i b·ªè t∆∞∆°ng quan v·ªõi ch√≠nh n√≥)
# Xem xet c√≥ c·∫ßn ƒë∆∞a v√†o ch∆∞∆°ng tr√¨nh?
high_corr_threshold = 0.8
high_corr_pairs = {}
totalVariant = 0
# L·∫∑p qua ma tr·∫≠n t∆∞∆°ng quan ch·ªâ ·ªü tam gi√°c d∆∞·ªõi ƒë·ªÉ tr√°nh l·∫∑p v√† t∆∞∆°ng quan ch√≠nh n√≥
for i in range(len(correlation_matrix.columns)):
    for j in range(i + 1, len(correlation_matrix.columns)):
        col1 = correlation_matrix.columns[i]
        col2 = correlation_matrix.columns[j]
        corr_value = correlation_matrix.iloc[i, j]
        totalVariant = totalVariant + 1
        if abs(corr_value) >= high_corr_threshold:
            high_corr_pairs[(col1, col2)] = corr_value

if high_corr_pairs:
    print(
        f"\n T·ªïng s·ªë c·∫∑p bi·∫øn :{len(high_corr_pairs)} c√≥ |r| >= {high_corr_threshold} tr√™n t·ªïng s·ªë c·∫∑p bi·∫øn : {totalVariant}"
    )
    print(f"\nC√°c c·∫∑p bi·∫øn c√≥ t∆∞∆°ng quan |r| >= {high_corr_threshold}:")
    for pair, corr in high_corr_pairs.items():
        print(f"  {pair[0]} v√† {pair[1]}: {corr:.4f}")
    print("\nNh·∫≠n x√©t: C√°c c·∫∑p bi·∫øn n√†y c√≥ m·ªëi quan h·ªá tuy·∫øn t√≠nh m·∫°nh, c√≥ th·ªÉ ch·ª©a th√¥ng tin tr√πng l·∫∑p.")
    print("PCA s·∫Ω hi·ªáu qu·∫£ trong vi·ªác k·∫øt h·ª£p c√°c bi·∫øn n√†y th√†nh √≠t th√†nh ph·∫ßn h∆°n.")
else:
    print(f"\nKh√¥ng t√¨m th·∫•y c·∫∑p bi·∫øn n√†o c√≥ t∆∞∆°ng quan |r| >= {high_corr_threshold}.")
    print(
        "Nh·∫≠n x√©t: C√°c bi·∫øn c√≥ v·∫ª t∆∞∆°ng ƒë·ªëi ƒë·ªôc l·∫≠p theo nghƒ©a tuy·∫øn t√≠nh. PCA v·∫´n c√≥ th·ªÉ gi√∫p t√¨m c√°c th√†nh ph·∫ßn ch√≠nh, nh∆∞ng m·ª©c ƒë·ªô gi·∫£m chi·ªÅu c√≥ th·ªÉ kh√¥ng l·ªõn n·∫øu kh√¥ng c√≥ nh√≥m bi·∫øn t∆∞∆°ng quan cao."
    )

# 2.5.2 Ki·ªÉm tra/T√¨m c√°c c·∫∑p bi·∫øn c√≥ t∆∞∆°ng quan th·∫•p (v√≠ d·ª•: |r| < 0.3)
low_corr_threshold = 0.3
low_corr_pairs = {}
for i in range(len(correlation_matrix.columns)):
    for j in range(i + 1, len(correlation_matrix.columns)):
        col1 = correlation_matrix.columns[i]
        col2 = correlation_matrix.columns[j]
        corr_value = correlation_matrix.iloc[i, j]

        if abs(corr_value) < low_corr_threshold:
            low_corr_pairs[(col1, col2)] = corr_value

# Ch·ªâ in m·ªôt v√†i c·∫∑p ti√™u bi·ªÉu n·∫øu s·ªë l∆∞·ª£ng qu√° l·ªõn
num_low_corr_to_print = 10
if low_corr_pairs:
    print(f"\nM·ªôt v√†i c·∫∑p bi·∫øn c√≥ t∆∞∆°ng quan |r| < {low_corr_threshold} (ti√™u bi·ªÉu {num_low_corr_to_print} c·∫∑p):")
    for i, (pair, corr) in enumerate(low_corr_pairs.items()):
        if i >= num_low_corr_to_print:
            break
        print(f"  {pair[0]} v√† {pair[1]}: {corr:.4f}")
    print(
        "\nNh·∫≠n x√©t: C√°c c·∫∑p bi·∫øn n√†y c√≥ m·ªëi quan h·ªá tuy·∫øn t√≠nh y·∫øu. Ch√∫ng c√≥ th·ªÉ ƒë·∫°i di·ªán cho c√°c y·∫øu t·ªë ƒë·ªôc l·∫≠p trong d·ªØ li·ªáu."
    )
else:
    print(f"\nKh√¥ng t√¨m th·∫•y c·∫∑p bi·∫øn n√†o c√≥ t∆∞∆°ng quan |r| < {low_corr_threshold}.")

# 2.5.4 Ki·ªÉm tra ph∆∞∆°ng sai ƒë·ªÉ x√°c ƒë·ªãnh ng∆∞·ª°ng (tr√™n X_Variant)
# Ng∆∞·ª°ng n√™n ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh d·ª±a tr√™n ph√¢n ph·ªëi ph∆∞∆°ng sai c·ªßa c√°c bi·∫øn trong covariance_matrix.
# Ki·ªÉm tra ph√¢n ph·ªëi n√†y b·∫±ng c√°ch in ra variances = np.diag(correlation_matrix) v√† ch·ªçn ng∆∞·ª°ng d·ª±a tr√™n ph√¢n v·ªã
# (v√≠ d·ª•: 5th percentile ho·∫∑c 10th percentile) c√≥ th·ªÉ thay ƒë·ªïi
variances = np.diag(covariance_matrix)
percentile_threshold = 5
# Set low variance threshold - adjust based on percentile if needed

low_variance_threshold = np.percentile(variances, percentile_threshold)
print(f"\nPh∆∞∆°ng sai nh·ªè nh·∫•t (tr√™n X_Variant): {variances.min():.4f}")
print(f"Ph∆∞∆°ng sai trung b√¨nh (tr√™n X_Variant): {variances.mean():.4f}")
print(
    f"Ph√¢n v·ªã {percentile_threshold}% c·ªßa variances (tr√™n X_Variant): {np.percentile(variances, percentile_threshold):.4f}"
)
print(f"Ng∆∞·ª°ng ph∆∞∆°ng sai th·∫•p ƒë∆∞·ª£c s·ª≠ d·ª•ng (d·ª±a tr√™n {percentile_threshold}% ph√¢n v·ªã): {low_variance_threshold:.4f}")


# 3. D√πng pca.fit(X_Variant) ƒë·ªÉ ph√¢n t√≠ch ban ƒë·∫ßu - Ch·ªâ hu·∫•n luy·ªán m√¥ h√¨nh
pca = PCA()
if not X_Variant.empty:
    pca.fit(X_Variant)

    # 3.1 T√≠nh eigenvalues (ph∆∞∆°ng sai c·ªßa t·ª´ng th√†nh ph·∫ßn ch√≠nh)
    eigenvalues = pca.explained_variance_
    print("\nEigenvalues (ph∆∞∆°ng sai c·ªßa t·ª´ng PC):", eigenvalues[:5])
    print(
        "√ù nghƒ©a: Eigenvalues th·ªÉ hi·ªán ph∆∞∆°ng sai m√† m·ªói th√†nh ph·∫ßn ch√≠nh gi·∫£i th√≠ch. Gi√° tr·ªã c√†ng l·ªõn, PC c√†ng quan tr·ªçng."
    )

    # 3.2 T√≠nh eigenvectors (c√°c vector ri√™ng, ƒë·ªãnh h∆∞·ªõng c·ªßa PC)
    eigenvectors = pca.components_
    print("\nEigenvectors (h∆∞·ªõng c·ªßa PC1):", eigenvectors[0][:5])
    print("√ù nghƒ©a: Eigenvectors ƒë·ªãnh h∆∞·ªõng cho m·ªói PC, th·ªÉ hi·ªán m·ª©c ƒë·ªô ƒë√≥ng g√≥p c·ªßa t·ª´ng bi·∫øn g·ªëc v√†o PC.")

    # 3.3 T√≠nh t·ª∑ l·ªá ph∆∞∆°ng sai gi·∫£i th√≠ch
    ##L√† m·ª©c ƒë·ªô ph∆∞∆°ng sai trong d·ªØ li·ªáu g·ªëc m√† m·ªói th√†nh ph·∫ßn ch√≠nh (PC) gi·∫£i th√≠ch ƒë∆∞·ª£c.
    explained_variance_ratio = pca.explained_variance_ratio_
    print("\nT·ª∑ l·ªá ph∆∞∆°ng sai gi·∫£i th√≠ch dem l·∫°i gia tr·ªã n√†y:", explained_variance_ratio[:5])

    # 3.4 T√≠nh t·ª∑ l·ªá ph∆∞∆°ng sai t√≠ch l≈©y
    ##L√† t·ªïng c·ªông d·ªìn c·ªßa c√°c ph∆∞∆°ng sai gi·∫£i th√≠ch t·ª´ PC‚ÇÅ ƒë·∫øn PC‚Çô. N√≥ cho bi·∫øt t·ªïng l∆∞·ª£ng th√¥ng tin (ph∆∞∆°ng sai) ƒë∆∞·ª£c gi·ªØ l·∫°i khi ch·ªçn ùëõ th√†nh ph·∫ßn ch√≠nh ƒë·∫ßu ti√™n.
    cumulative_variance = np.cumsum(explained_variance_ratio)

    print("\n--- Ph√¢n t√≠ch PCA ---")
    print("T·ª∑ l·ªá ph∆∞∆°ng sai gi·∫£i th√≠ch b·ªüi t·ª´ng th√†nh ph·∫ßn ch√≠nh:")
    for i, ratio in enumerate(explained_variance_ratio):
        print(f"  PC{i+1}: {ratio:.4f} ({cumulative_variance[i]:.4f} t√≠ch l≈©y)")

    print("T·ª∑ l·ªá ph∆∞∆°ng sai t√≠ch l≈©y:", cumulative_variance[: min(5, len(cumulative_variance))])
    print(
        "√ù nghƒ©a: T·ª∑ l·ªá ph∆∞∆°ng sai t√≠ch l≈©y cho bi·∫øt t·ªïng ph∆∞∆°ng sai ƒë∆∞·ª£c gi·∫£i th√≠ch b·ªüi c√°c PC. Th∆∞·ªùng c·∫ßn >= 80% ƒë·ªÉ ƒë·∫£m b·∫£o d·ªØ li·ªáu gi·∫£m chi·ªÅu t·ªët."
    )
    # ƒê√ÅNH GI√Å METRIC NAY C√ì TR√ôNG LƒÇP - START
    # 3.5.1 ƒê√°nh gi√° d·ª±a tr√™n ph∆∞∆°ng sai gi·∫£i th√≠ch
    # Ng∆∞·ª°ng ph∆∞∆°ng sai t√≠ch l≈©y variance_threshold (cumulative_variance_threshold) > 90%
    # Chu·∫©n ph·ªï bi·∫øn khi √°p d·ª•ng cho PCA + ARIMA / ARIMAX	Th·ªèa m√£n ƒë·ªô ch√≠nh x√°c & tr√°nh overfitting v·ªõi K·ª≥ v·ªçng S·ªë l∆∞·ª£ng PC ƒëi·ªÉn h√¨nh t·ª´ 5~7
    variance_threshold = 0.95
    n_components_to_keep = np.argmax(cumulative_variance >= variance_threshold) + 1
    print(f"\n--- ƒê√°nh gi√° PCA ---")
    print(f"T·ªïng s·ªë bi·∫øn g·ªëc (sau t√°ch VNINDEX): {X_Variant.shape[1]}")
    print(f"Ng∆∞·ª°ng ph∆∞∆°ng sai t√≠ch l≈©y mong mu·ªën: {variance_threshold*100:.0f}%")
    print(f"S·ªë th√†nh ph·∫ßn ch√≠nh PCA c√≥ th·ªÉ gi·ªØ l·∫°i: {n_components_to_keep}")
    print(f"\n--- ƒê√°nh gi√° PCA ti√™u ch√≠ 1 ---")
    # 3.5.1 ƒê·∫£m b·∫£o ng∆∞·ª°ng c√≥ th·ªÉ ƒë·∫°t ƒë∆∞·ª£c
    if n_components_to_keep <= X_Variant.shape[1]:
        print(
            f"ƒê·ªÉ gi·∫£i th√≠ch √≠t nh·∫•t {variance_threshold*100:.0f}% ph∆∞∆°ng sai, c·∫ßn {n_components_to_keep} th√†nh ph·∫ßn ch√≠nh."
        )
        reduction_percentage = (1 - n_components_to_keep / X_Variant.shape[1]) * 100
        print(
            f"C√≥ th·ªÉ gi·∫£m chi·ªÅu d·ªØ li·ªáu t·ª´ {X_Variant.shape[1]} xu·ªëng {n_components_to_keep} chi·ªÅu (gi·∫£m kho·∫£ng {reduction_percentage:.2f}%)."
        )
        print(
            "\nNh·∫≠n x√©t: PCA cho ph√©p gi·∫£m ƒë√°ng k·ªÉ s·ªë chi·ªÅu d·ªØ li·ªáu trong khi v·∫´n gi·ªØ l·∫°i ph·∫ßn l·ªõn th√¥ng tin (ph∆∞∆°ng sai). C√°c th√†nh ph·∫ßn ch√≠nh n√†y s·∫Ω l√† c√°c ƒë·∫∑c tr∆∞ng m·ªõi cho c√°c m√¥ h√¨nh ti·∫øp theo."
        )
    else:
        print(f"Ng∆∞·ª°ng ph∆∞∆°ng sai {variance_threshold*100:.0f}% kh√¥ng th·ªÉ ƒë·∫°t ƒë∆∞·ª£c ngay c·∫£ v·ªõi t·∫•t c·∫£ c√°c th√†nh ph·∫ßn.")
        print(f"T·ªïng ph∆∞∆°ng sai gi·∫£i th√≠ch t·ªëi ƒëa l√† {cumulative_variance[-1]*100:.2f}%.")
        print(
            "Nh·∫≠n x√©t: Vi·ªác gi·∫£m chi·ªÅu b·∫±ng PCA kh√¥ng hi·ªáu qu·∫£ nhi·ªÅu trong tr∆∞·ªùng h·ª£p n√†y n·∫øu m·ª•c ti√™u l√† gi·ªØ l·∫°i t·ª∑ l·ªá ph∆∞∆°ng sai r·∫•t cao."
        )

    print(f"\n--- ƒê√°nh gi√° PCA ti√™u ch√≠ 2 ---")

    if cumulative_variance[-1] >= variance_threshold:
        print(
            f"ƒê·ªÉ gi·∫£i th√≠ch √≠t nh·∫•t {variance_threshold*100:.0f}% ph∆∞∆°ng sai, c·∫ßn {n_components_to_keep} th√†nh ph·∫ßn ch√≠nh."
        )
        reduction_percentage = (1 - n_components_to_keep / X_Variant.shape[1]) * 100
        print(
            f"C√≥ th·ªÉ gi·∫£m chi·ªÅu d·ªØ li·ªáu t·ª´ {X_Variant.shape[1]} xu·ªëng {n_components_to_keep} chi·ªÅu (gi·∫£m kho·∫£ng {reduction_percentage:.2f}%)."
        )
        print(
            "\nNh·∫≠n x√©t: PCA cho ph√©p gi·∫£m ƒë√°ng k·ªÉ s·ªë chi·ªÅu d·ªØ li·ªáu trong khi v·∫´n gi·ªØ l·∫°i ph·∫ßn l·ªõn th√¥ng tin (ph∆∞∆°ng sai). C√°c th√†nh ph·∫ßn ch√≠nh n√†y s·∫Ω l√† c√°c ƒë·∫∑c tr∆∞ng m·ªõi cho c√°c m√¥ h√¨nh ti·∫øp theo."
        )
    else:
        print(
            f"Ngay c·∫£ khi gi·ªØ l·∫°i t·∫•t c·∫£ {X_Variant.shape[1]} th√†nh ph·∫ßn, t·ªïng ph∆∞∆°ng sai gi·∫£i th√≠ch l√† {cumulative_variance[-1]*100:.2f}%."
        )
        print("Nh·∫≠n x√©t: C√≥ th·ªÉ t·∫≠p d·ªØ li·ªáu n√†y kh√¥ng ch·ª©a nhi·ªÅu th√¥ng tin d∆∞ th·ª´a (bi·∫øn t∆∞∆°ng quan cao);")
        print(
            "          ho·∫∑c ph∆∞∆°ng sai ph√¢n b·ªï ƒë·ªÅu tr√™n nhi·ªÅu chi·ªÅu. Vi·ªác gi·∫£m chi·ªÅu b·∫±ng PCA c√≥ th·ªÉ kh√¥ng hi·ªáu qu·∫£ nhi·ªÅu trong tr∆∞·ªùng h·ª£p n√†y n·∫øu m·ª•c ti√™u l√† gi·ªØ l·∫°i t·ª∑ l·ªá ph∆∞∆°ng sai r·∫•t cao."
        )

    # 3.5.2 ƒê√°nh gi√°: 5 th√†nh ph·∫ßn ch√≠nh c√≥ ƒë·ªß t·ªët kh√¥ng?
    print(f"\n--- ƒê√°nh kh·∫£ nƒÉng gi·∫£m chi·ªÅu c√°c th√†nh ph·∫ßn ch√≠nh PCA. : ")
    if len(cumulative_variance) > 4:  # Check if 5th PC (index 4) exists
        if cumulative_variance[4] >= variance_threshold:
            print(
                f"{n_components_to_keep}  th√†nh ph·∫ßn ch√≠nh gi·∫£i th√≠ch {cumulative_variance[4]*100:.2f}% ph∆∞∆°ng sai, ƒë·ªß t·ªët ƒë·ªÉ gi·∫£m chi·ªÅu cho ARIMAX."
            )
        else:
            print(
                f"{n_components_to_keep} th√†nh ph·∫ßn ch√≠nh ch·ªâ gi·∫£i th√≠ch {cumulative_variance[4]*100:.2f}% ph∆∞∆°ng sai, c√≥ th·ªÉ c·∫ßn th√™m PC."
            )
    else:
        print(
            f"Ch·ªâ c√≥ {len(cumulative_variance)} th√†nh ph·∫ßn ch√≠nh. T·ªïng ph∆∞∆°ng sai gi·∫£i th√≠ch l√† {cumulative_variance[-1]*100:.2f}%."
        )

    print(f"\n--- K·∫øt lu·∫≠n kh·∫£ nƒÉng gi·∫£m chi·ªÅu PCA ---")
    print(f"T·ªïng s·ªë bi·∫øn g·ªëc: {df_standardized.shape[1]}")  # Includes VNINDEX here
    print(f"Ng∆∞·ª°ng ph∆∞∆°ng sai t√≠ch l≈©y mong mu·ªën: {variance_threshold*100:.0f}%")
    if len(cumulative_variance) > 4:
        print(f"Ng∆∞·ª°ng ph∆∞∆°ng sai t√≠ch l≈©y ƒë·∫°t ƒë∆∞·ª£c (v·ªõi 5 PC): {cumulative_variance[4]*100:.0f}%")
    else:
        print("Kh√¥ng ƒë·ªß 5 th√†nh ph·∫ßn ch√≠nh.")

    print(f"S·ªë th√†nh ph·∫ßn ch√≠nh PCA c√≥ th·ªÉ gi·ªØ l·∫°i theo ng∆∞·ª°ng {variance_threshold*100:.0f}%: {n_components_to_keep}")
    # -- END
    # 3.6 Bi·ªÉu ƒë·ªì t·ª∑ l·ªá ph∆∞∆°ng sai t√≠ch l≈©y
    # Tr·ª±c quan h√≥a T·ª∑ l·ªá Ph∆∞∆°ng sai Gi·∫£i th√≠ch
    plt.figure(figsize=(10, 6))
    plt.plot(
        range(1, len(explained_variance_ratio) + 1),
        cumulative_variance,
        marker="o",
        linestyle="--",
    )
    plt.xlabel("S·ªë l∆∞·ª£ng th√†nh ph·∫ßn ch√≠nh")
    plt.ylabel("T·ªïng Ph∆∞∆°ng sai Gi·∫£i th√≠ch (%)")
    plt.title("Bi·ªÉu ƒë·ªì Ph∆∞∆°ng sai Gi·∫£i th√≠ch T√≠ch l≈©y c·ªßa PCA")
    plt.xticks(range(1, len(explained_variance_ratio) + 1))
    plt.grid(True)
    plt.savefig("pca_explained_variance.png")
    plt.show()
    plt.close()
    try:
        files.download("pca_explained_variance.png")
        print("\nƒê√£ l∆∞u bi·ªÉu ƒë·ªì ph∆∞∆°ng sai gi·∫£i th√≠ch t√≠ch l≈©y v√†o 'pca_explained_variance.png'")
    except NameError:
        print("\nRunning in a non-Colab environment, skipping file downloads.")
        print("Plot saved locally: pca_explained_variance.png")

    # 4. T√≠nh to√°n v√† l·ªçc d·ªØ li·ªáu d·ª±a tr√™n ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai, ma tr·∫≠n t∆∞∆°ng quan v√† loadings
    # T√≠nh vector t∆∞∆°ng quan (loadings)    # T√≠nh vector t∆∞∆°ng quan (loadings) tr√™n X_Variant
    if pca.n_components_ > 0:
        loadings = pca.components_.T * np.sqrt(pca.explained_variance_)
        loadings_df = pd.DataFrame(
            loadings, columns=[f"PC{i+1}" for i in range(loadings.shape[1])], index=X_Variant.columns
        )
        print("\nLoadings cho PC1 (ƒë√£ ƒë∆∞·ª£c s·∫Øp x·∫øp gi·∫£m d·∫ßn):")
        if "PC1" in loadings_df.columns:
            print(loadings_df["PC1"].sort_values(ascending=False).head(min(5, len(loadings_df["PC1"]))))
            print("√ù nghƒ©a: Loadings th·ªÉ hi·ªán m·ª©c ƒë·ªô ƒë√≥ng g√≥p c·ªßa c√°c bi·∫øn g·ªëc v√†o PC1.")
        else:
            print("PC1 does not exist in loadings_df. Cannot print loadings.")
    else:
        print("\nPCA resulted in 0 components on X_Variant. Cannot calculate loadings.")

    # 4.1 Lo·∫°i nhi·ªÖu: D·ª±a tr√™n ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai (ph∆∞∆°ng sai th·∫•p) v√† loadings (ƒë√≥ng g√≥p th·∫•p)
    # Identify low variance variables based on the calculated threshold
    low_variance_vars = covariance_matrix.columns[variances < low_variance_threshold].tolist()
    print(f"\nBi·∫øn c√≥ ph∆∞∆°ng sai th·∫•p (d·ª±a tr√™n ng∆∞·ª°ng {low_variance_threshold:.4f}): {low_variance_vars}")
    ##Ng∆∞·ª°ng trung b√¨nh tuy·ªát ƒë·ªëi c·ªßa loadings (vector t∆∞∆°ng quan) ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ x√°c ƒë·ªãnh c√°c bi·∫øn c√≥ ƒë√≥ng g√≥p th·∫•p v√†o c√°c th√†nh ph·∫ßn ch√≠nh (PCs) v√† c√≥ th·ªÉ b·ªã coi l√† nhi·ªÖu.
    ##Gi√° tr·ªã n√†y ƒë·∫°i di·ªán cho m·ª©c ƒë·ªô quan tr·ªçng trung b√¨nh c·ªßa m·ªôt bi·∫øn trong vi·ªác gi·∫£i th√≠ch ph∆∞∆°ng sai c·ªßa d·ªØ li·ªáu qua t·∫•t c·∫£ c√°c PCs.
    ##N·∫øu trung b√¨nh tuy·ªát ƒë·ªëi c·ªßa loadings c·ªßa m·ªôt bi·∫øn nh·ªè h∆°n 0.1, bi·∫øn ƒë√≥ ƒë∆∞·ª£c cho l√† kh√¥ng ƒë·ªß √Ω nghƒ©a v√† c√≥ th·ªÉ b·ªã lo·∫°i b·ªè.
    ##Ng∆∞·ª°ng low_contribution_threshold = 0.1 l√† h·ª£p l√Ω cho d·ªØ li·ªáu chu·∫©n h√≥a, c√≥ th·ªÉ ki·ªÉm tra ph√¢n ph·ªëi mean_loadings ƒë·ªÉ ƒëi·ªÅu ch·ªânh (tƒÉng l√™n 0.2 ho·∫∑c gi·∫£m xu·ªëng 0.05 n·∫øu c·∫ßn).

    # Identify low contribution variables based on loadings (on X_Variant)
    # Determine a threshold based on the distribution of mean loadings
    if "loadings_df" in locals() and not loadings_df.empty:
        mean_loadings = np.abs(loadings_df).mean(axis=1)
        # Example: Keep top 80% variables by mean loading
        # Keep variables above the 20th => 30 percentile of mean loadings
        # TƒÉng ng∆∞·ª°ng ƒë√≥ng g√≥p th·∫•p t·ª´ ph√¢n v·ªã 20% l√™n 30% ƒë·ªÉ gi·ªØ l·∫°i nhi·ªÅu bi·∫øn h∆°n, tr√°nh lo·∫°i b·ªè c√°c bi·∫øn quan tr·ªçng
        percentile_loading_threshold = 30
        low_contribution_threshold = np.percentile(mean_loadings, percentile_loading_threshold)
        print(
            f"Ng∆∞·ª°ng ƒë√≥ng g√≥p th·∫•p ƒë∆∞·ª£c s·ª≠ d·ª•ng (d·ª±a tr√™n {percentile_loading_threshold}th percentile c·ªßa mean loadings): {low_contribution_threshold:.4f}"
        )

        low_contribution_vars = mean_loadings[mean_loadings < low_contribution_threshold].index.tolist()
        print(f"S·ªë bi·∫øn c√≥ mean_loadings < {low_contribution_threshold:.4f}: {len(low_contribution_vars)}")
        print(f"Danh s√°ch mean_loadings th·∫•p nh·∫•t:\n{mean_loadings.sort_values().head(10)}")  # Print more if needed
    else:
        print("loadings_df is not available or is empty. Skipping low contribution variable check.")
        low_contribution_vars = []  # Ensure this list is empty if loadings_df is not available

    # Combine low variance and low contribution variables to identify noise
    noisy_variables = list(set(low_variance_vars + low_contribution_vars))
    print(f"\nBi·∫øn nhi·ªÖu b·ªã lo·∫°i b·ªè: {noisy_variables}")

    # Create df_cleaned_noise by dropping noisy variables from df_standardized
    df_cleaned_noise = df_standardized.drop(columns=noisy_variables)
    print(f"S·ªë l∆∞·ª£ng bi·∫øn sau khi l·ªçc nhi·ªÖu: {df_cleaned_noise.shape[1]}")  # Includes VNINDEX potentially

    # 4.2 Lo·∫°i b·ªè bi·∫øn d∆∞ th·ª´a: D·ª±a tr√™n ma tr·∫≠n t∆∞∆°ng quan Pearson (t∆∞∆°ng quan > 0.8) on df_cleaned_noise
    variables_to_drop_redundant = set()
    # Only proceed if there are enough columns for correlation matrix after noise cleaning
    if not df_cleaned_noise.empty and df_cleaned_noise.shape[1] > 1:
        # Ensure 'VNINDEX' is excluded from redundant variable check
        df_cleaned_noise_features = (
            df_cleaned_noise.drop(columns=["VNINDEX"])
            if "VNINDEX" in df_cleaned_noise.columns
            else df_cleaned_noise.copy()
        )

        if not df_cleaned_noise_features.empty and df_cleaned_noise_features.shape[1] > 1:
            corr_matrix_cleaned = df_cleaned_noise_features.corr()
            highly_correlated_pairs_cleaned = []
            for i in range(len(corr_matrix_cleaned.columns)):
                for j in range(i + 1, len(corr_matrix_cleaned.columns)):
                    if (
                        abs(corr_matrix_cleaned.iloc[i, j]) > high_corr_threshold
                    ):  # Use the high_corr_threshold defined earlier
                        highly_correlated_pairs_cleaned.append(
                            (
                                corr_matrix_cleaned.columns[i],
                                corr_matrix_cleaned.columns[j],
                                corr_matrix_cleaned.iloc[i, j],
                            )
                        )

            for var1, var2, corr in highly_correlated_pairs_cleaned:
                # Decide which one to drop - here we drop var2 from the redundant pair
                # Ensure the variable is not the target variable
                if var1 != "VNINDEX" and var2 != "VNINDEX":
                    # To avoid adding the same variable multiple times if it's correlated with several others
                    if var2 not in variables_to_drop_redundant:
                        variables_to_drop_redundant.add(var2)

            df_cleaned = df_cleaned_noise.drop(columns=list(variables_to_drop_redundant))
            print(
                f"\nBi·∫øn d∆∞ th·ª´a b·ªã lo·∫°i b·ªè (d·ª±a tr√™n ma tr·∫≠n Pearson tr√™n d·ªØ li·ªáu ƒë√£ l·ªçc nhi·ªÖu): {list(variables_to_drop_redundant)}"
            )
        else:
            print("\nKh√¥ng ƒë·ªß bi·∫øn (sau l·ªçc nhi·ªÖu v√† lo·∫°i VNINDEX) ƒë·ªÉ ki·ªÉm tra bi·∫øn d∆∞ th·ª´a.")
            df_cleaned = df_cleaned_noise.copy()  # Keep the same DataFrame
    else:
        print("\ndf_cleaned_noise is empty or has only one column. Skipping redundant variable check.")
        df_cleaned = df_cleaned_noise.copy()  # Keep the same DataFrame

else:  # Case where initial X_Variant was empty
    print("Initial X_Variant DataFrame is empty. Cannot proceed with PCA analysis and filtering.")
    # Set df_cleaned to an empty state or handle appropriately
    df_cleaned = pd.DataFrame(index=df_standardized.index)
    explained_variance_ratio = np.array([])
    cumulative_variance = np.array([])
    n_components_to_keep = 0
    # Skip subsequent steps that depend on PCA and filtering


# 5.Pha gi·∫£m chi·ªÅu  √Åp d·ª•ng PCA tr√™n d·ªØ li·ªáu ƒë√£ l·ªçc v√† lo·∫°i b·ªè bi·∫øn m·ª•c ti√™u VNINDEX
# Ensure df_cleaned has columns and contains VNINDEX before proceeding
X_cleaned = pd.DataFrame()
y = pd.Series()
X_pca_df = pd.DataFrame()  # Initialize X_pca_df

if "VNINDEX" in df_cleaned.columns and df_cleaned.shape[1] > 1:  # Need VNINDEX and at least one feature for PCA
    X_cleaned = df_cleaned.drop(columns=["VNINDEX"])
    y = df_cleaned["VNINDEX"]
    print(f"\nShape of X_cleaned before final PCA: {X_cleaned.shape}")

    # Check if X_cleaned is empty before applying final PCA
    if not X_cleaned.empty and X_cleaned.shape[1] > 0:
        # 5.1 Verify c√°c S·ªë th√†nh ph·∫ßn ch√≠nh
        # Ensure n_components_to_keep is valid for X_cleaned dimensions
        # Decide on the number of components for the final PCA
        # Option 1: Use n_components_to_keep from the initial PCA based on the cumulative variance threshold
        # Option 2: Choose a fixed number, e.g., 5, if that's the project requirement
        # Let's use 5 as the user's last code snippet intended, but ensure it doesn't exceed the number of features
        # Use 5 PCs, but no more than the available features
        # variance_threshold = 0.95 => C√≥ 7 PC s·ª≠ d·ª•ng gi√° tr·ªã suy lu·∫≠n t·ª´ variance_threshold l√† n_components_to_keep ƒë·ªÉ t√≠nh to√°n
        n_components_final_pca = min(n_components_to_keep, X_cleaned.shape[1])

        if n_components_final_pca > 0:
            print(f"Applying final PCA with n_components = {n_components_final_pca}")
            pca_final = PCA(n_components=n_components_final_pca)
            X_pca_final = pca_final.fit_transform(X_cleaned)

            # T·∫°o DataFrame cho c√°c th√†nh ph·∫ßn ch√≠nh
            X_pca_df = pd.DataFrame(
                X_pca_final, columns=[f"PC{i+1}" for i in range(n_components_final_pca)], index=df_cleaned.index
            )

            # T√≠nh l·∫°i t·ª∑ l·ªá ph∆∞∆°ng sai t√≠ch l≈©y sau khi l·ªçc
            explained_variance_ratio_final = pca_final.explained_variance_ratio_ * 100
            cumulative_variance_final = np.cumsum(explained_variance_ratio_final)

            # T√≠nh loadings sau khi l·ªçc
            loadings_final = pca_final.components_.T * np.sqrt(pca_final.explained_variance_)
            loadings_final_df = pd.DataFrame(
                loadings_final, columns=[f"PC{i+1}" for i in range(n_components_final_pca)], index=X_cleaned.columns
            )
            ###===== B·ªï sung
            ### 5.1.1 v·∫Ω bi·ªÉu ƒë·ªì histogram, boxplot, ho·∫∑c ki·ªÉm tra skewness/kurtosis ƒë·ªÉ b·ªï sung ph√¢n t√≠ch thƒÉm d√≤.

            # Thi·∫øt l·∫≠p ki·ªÉu d√°ng cho bi·ªÉu ƒë·ªì v·ªõi h√¨nh n·ªÅn tr·∫Øng
            # S·ª≠ d·ª•ng style 'white' ho·∫∑c 'whitegrid'
            #sns.set_style('whitegrid')  # Set the style to 'white' for a plain white background
            # If you still want subtle grid lines, use 'whitegrid' instead:
            sns.set_style('whitegrid')
            sns.set_context('notebook')
            sns.set(font_scale=1.2)

            # The rest of your code remains the same:
            # ƒê·ªçc d·ªØ li·ªáu ƒë√£ chu·∫©n h√≥a
            #df_standardized = pd.read_csv("VNINDEX_robust_standardized.csv", index_col="Day", parse_dates=True)

            # Danh s√°ch bi·∫øn ngo·∫°i sinh sau khi l·ªçc (df_cleaned)
            # Make sure 'VNINDEX' is included if you plan to plot it as the target
            selected_vars = df_cleaned

            # L·ªçc d·ªØ li·ªáu ƒë·ªÉ ch·ªâ l·∫•y c√°c bi·∫øn c·∫ßn ph√¢n t√≠ch
            # Ensure selected_vars contains only columns present in df_standardized
            actual_selected_vars = [col for col in selected_vars if col in df_standardized.columns]
            df_eda = df_standardized[actual_selected_vars]

            if df_eda.empty:
                print("Warning: The filtered DataFrame for EDA is empty. Check 'selected_vars' and 'df_standardized.columns'.")
            else:
                # 1. T√≠nh to√°n skewness v√† kurtosis
                stats_table = []
                for col in df_eda.columns:
                    # Ensure there are non-NaN values to compute stats
                    if df_eda[col].dropna().shape[0] > 1:
                        skewness = skew(df_eda[col].dropna())
                        kurt = kurtosis(df_eda[col].dropna(), fisher=True)  # Fisher=True ƒë·ªÉ kurtosis chu·∫©n h√≥a (0 cho ph√¢n ph·ªëi chu·∫©n)
                        stats_table.append([col, f"{skewness:.4f}", f"{kurt:.4f}"])
                    else:
                        stats_table.append([col, "N/A", "N/A"]) # Handle columns with all NaNs

                print("\nTh·ªëng k√™ Skewness v√† Kurtosis c·ªßa c√°c bi·∫øn:")
                print(tabulate(stats_table, headers=["Bi·∫øn", "Skewness", "Kurtosis"], tablefmt="grid", numalign="right"))

                # 2. V·∫Ω bi·ªÉu ƒë·ªì Histogram
                # Adjust grid size based on the number of selected variables
                n_vars = len(df_eda.columns)
                n_cols = 3
                n_rows = (n_vars + n_cols - 1) // n_cols # Calculate rows needed

                plt.figure(figsize=(n_cols * 5, n_rows * 4)) # Adjust figure size dynamically
                for i, col in enumerate(df_eda.columns, 1):
                    plt.subplot(n_rows, n_cols, i)
                    # Only plot if there's data
                    if df_eda[col].dropna().shape[0] > 0:
                        sns.histplot(df_eda[col].dropna(), bins=30, kde=True, color='blue')
                        plt.title(f'Histogram c·ªßa {col}')
                        plt.xlabel(col)
                        plt.ylabel('T·∫ßn s·ªë')
                    else:
                        plt.title(f'Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ v·∫Ω Histogram cho {col}')
                        # Optionally plot an empty subplot or text
                        plt.text(0.5, 0.5, 'No data', horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)
                plt.savefig("eda_histogram.png", dpi=300)
                plt.show()
                plt.close()

                try:
                    if files:
                        files.download("eda_histogram.png")
                        print("ƒê√£ l∆∞u bi·ªÉu ƒë·ªì histogram v√†o 'eda_histogram.png'")
                except NameError:
                    print("Ch·∫°y trong m√¥i tr∆∞·ªùng kh√¥ng ph·∫£i Colab, b·ªè qua t·∫£i file.")
                    print("Bi·ªÉu ƒë·ªì ƒë√£ ƒë∆∞·ª£c l∆∞u c·ª•c b·ªô: eda_histogram.png")

                # 3. V·∫Ω bi·ªÉu ƒë·ªì Boxplot
                plt.figure(figsize=(15, 8))
                # Ensure the DataFrame for boxplot is not empty
                if not df_eda.empty:
                    sns.boxplot(data=df_eda, palette="Set2")
                    plt.title("Boxplot c·ªßa c√°c bi·∫øn (chu·∫©n h√≥a)")
                    plt.xticks(rotation=45)
                    plt.ylabel("Gi√° tr·ªã chu·∫©n h√≥a")
                    plt.savefig("eda_boxplot.png", dpi=300)
                    plt.show()
                    plt.close()

                    try:
                        if files:
                            files.download("eda_boxplot.png")
                            print("ƒê√£ l∆∞u bi·ªÉu ƒë·ªì boxplot v√†o 'eda_boxplot.png'")
                    except NameError:
                        print("Ch·∫°y trong m√¥i tr∆∞·ªùng kh√¥ng ph·∫£i Colab, b·ªè qua t·∫£i file.")
                        print("Bi·ªÉu ƒë·ªì ƒë√£ ƒë∆∞·ª£c l∆∞u c·ª•c b·ªô: eda_boxplot.png")
                else:
                    print("\nKh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ v·∫Ω Boxplot.")


                # 4. V·∫Ω bi·ªÉu ƒë·ªì chu·ªói th·ªùi gian cho VNINDEX
                # Check if VNINDEX column exists in the filtered df_eda
                if 'VNINDEX' in df_eda.columns and not df_eda['VNINDEX'].empty:
                    plt.figure(figsize=(12, 6))
                    plt.plot(df_eda.index, df_eda['VNINDEX'], label='VNINDEX (chu·∫©n h√≥a)', color='blue')
                    plt.title("Chu·ªói th·ªùi gian VNINDEX (chu·∫©n h√≥a)")
                    plt.xlabel("Ng√†y")
                    plt.ylabel("Gi√° tr·ªã chu·∫©n h√≥a")
                    plt.legend()
                    plt.grid(True)
                    plt.xticks(rotation=45)
                    plt.savefig("eda_timeseries_vnindex.png", dpi=300)
                    plt.show()
                    plt.close()

                    try:
                        if files:
                            files.download("eda_timeseries_vnindex.png")
                            print("ƒê√£ l∆∞u bi·ªÉu ƒë·ªì chu·ªói th·ªùi gian v√†o 'eda_timeseries_vnindex.png'")
                    except NameError:
                        print("Ch·∫°y trong m√¥i tr∆∞·ªùng kh√¥ng ph·∫£i Colab, b·ªè qua t·∫£i file.")
                        print("Bi·ªÉu ƒë·ªì ƒë√£ ƒë∆∞·ª£c l∆∞u c·ª•c b·ªô: eda_timeseries_vnindex.png")
                else:
                    print("\nKh√¥ng t√¨m th·∫•y c·ªôt 'VNINDEX' trong d·ªØ li·ªáu ƒë√£ l·ªçc ho·∫∑c d·ªØ li·ªáu tr·ªëng. Kh√¥ng th·ªÉ v·∫Ω bi·ªÉu ƒë·ªì chu·ªói th·ªùi gian VNINDEX.")

            # 5. In b·∫£ng eigenvalues chi ti·∫øt
            eigenvalues_table = []
            for i in range(7):
                eigenvalues_table.append([f'PC{i+1}', f'{eigenvalues[i]:.4f}', f'{explained_variance_ratio_final[i]:.2f}%', f'{cumulative_variance_final[i]:.2f}%'])
            print("\nB·∫£ng Eigenvalues v√† T·ª∑ l·ªá Ph∆∞∆°ng sai:")
            print(tabulate(eigenvalues_table, headers=["Th√†nh ph·∫ßn", "Eigenvalue", "T·ª∑ l·ªá (%)", "T√≠ch l≈©y (%)"], tablefmt="grid", numalign="right"))

            # 2. V·∫Ω bi·ªÉu ƒë·ªì loadings c·ªßa 7 PC
            plt.figure(figsize=(15, 10))
            for i in range(7):
                plt.subplot(4, 2, i+1)  # S·∫Øp x·∫øp trong l∆∞·ªõi 4x2
                sns.barplot(x=loadings_final_df.index, y=loadings_final_df[f'PC{i+1}'], palette="viridis")
                plt.title(f'Loadings c·ªßa PC{i+1}')
                plt.xlabel('Bi·∫øn')
                plt.ylabel('H·ªá s·ªë Loadings')
                plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig("loadings_final.png", dpi=300)
            plt.show()
            plt.close()
            try:
                if files:
                    files.download("loadings_final.png")
                    print("ƒê√£ l∆∞u bi·ªÉu ƒë·ªì loadings v√†o 'loadings_final.png'")
            except NameError:
                print("Ch·∫°y trong m√¥i tr∆∞·ªùng kh√¥ng ph·∫£i Colab, b·ªè qua t·∫£i file.")
                print("Bi·ªÉu ƒë·ªì ƒë√£ ƒë∆∞·ª£c l∆∞u c·ª•c b·ªô: loadings_final.png")

        # 5.1.2 - CHeck In b·∫£ng k·∫øt qu·∫£ PC1~PC(n) v·ªõi top 5/7 bi·∫øn v√† p-value ADF
        # for i in range(n_components_final_pca):
        #  print(f"PC{i+1}: {explained_variance_ratio_final[i]:.2f}% ({cumulative_variance_final[i]:.2f}% t√≠ch l≈©y)")
        # --- thay th·∫ø b·∫±ng b·∫£ng chi ti·∫øt
        table_data = []
        for i in range(n_components_final_pca):
            # T√≠nh top 5 bi·∫øn ƒë√≥ng g√≥p
            abs_loadings = np.abs(loadings_final_df[f"PC{i+1}"])
            total_abs_loadings = abs_loadings.sum()
            #top_pc_vars = abs_loadings.sort_values(ascending=False).head(n_components_final_pca).index
            #top_pc_contrib = [
            #    f"{var} ({(abs_loadings[var] / total_abs_loadings * 100):.2f}%)" for var in top_pc_vars
            #]
            #top_pc_str = ", ".join(top_pc_contrib)

            # Ki·ªÉm tra ADF cho PC
            adf_result = adfuller(X_pca_df[f"PC{i+1}"])
            p_value = adf_result[1]

            table_data.append(
                [
                    f"PC{i+1}",
                    f"{explained_variance_ratio_final[i]:.2f}%",
                    f"{cumulative_variance_final[i]:.2f}%",
                    #top_pc_str,
                    f"{p_value:.4f}",
                ]
            )

        print(f"\nB·∫£ng th√¥ng tin c√°c th√†nh ph·∫ßn ch√≠nh (PC1~PC{n_components_final_pca}):")
        print(
            tabulate(
                table_data,
                headers=[
                    "Th√†nh ph·∫ßn ch√≠nh",
                    "Ph∆∞∆°ng sai gi·∫£i th√≠ch (%)",
                    "Ph∆∞∆°ng sai t√≠ch l≈©y (%)",
                    #f"Top {n_components_final_pca} bi·∫øn ƒë√≥ng g√≥p (% ƒë√≥ng g√≥p)",
                    "p-value ADF",
                ],
                numalign="right",
                stralign="center",
                maxcolwidths=[20, 20, 20, 100],
                tablefmt="grid",
            )
        )

        if cumulative_variance_final[-1] >= 80:
            # Check the last cumulative variance against a reasonable threshold
            print(
                f"√ù nghƒ©a T·ª∑ l·ªá ph∆∞∆°ng sai t√≠ch l≈©y {cumulative_variance_final[-1]:.2f}% tr√™n d·ªØ li·ªáu ƒë√£ l·ªçc v√† gi·∫£m chi·ªÅu l√† ph√π h·ª£p ƒë·ªÉ s·ª≠ d·ª•ng trong ARIMAX."
            )
        else:
            print(
                f"√ù nghƒ©a T·ª∑ l·ªá ph∆∞∆°ng sai t√≠ch l≈©y {cumulative_variance_final[-1]:.2f}% sau l·ªçc/gi·∫£m chi·ªÅu th·∫•p h∆°n 80%."
            )

        # 5.3Ki·ªÉm tra n_components_final_pca th√†nh ph·∫ßn ch√≠nh gi·∫£i th√≠ch c√≥ ƒë·ªß t·ªët
        print(
            f"\nKi·ªÉm tra s·ªë th√†nh ph√¢n ch√≠nh cho gi·∫£m chi·ªÅu t·∫≠p train {n_components_final_pca} th√†nh ph·∫ßn ch√≠nh gi·∫£i th√≠ch c√≥ ƒë·ªß t·ªët"
        )
        if len(cumulative_variance) > n_components_to_keep - 1:
            # Check if 7th PC (index 6) exists
            if cumulative_variance[n_components_final_pca - 1] >= variance_threshold:
                print(
                    f"{n_components_final_pca} th√†nh ph·∫ßn ch√≠nh gi·∫£i th√≠ch {cumulative_variance[n_components_final_pca-1]*100:.2f}% ph∆∞∆°ng sai, ƒë·ªß t·ªët ƒë·ªÉ gi·∫£m chi·ªÅu cho ARIMAX."
                )
            else:
                print(
                    f"{n_components_final_pca} th√†nh ph·∫ßn ch√≠nh ch·ªâ gi·∫£i th√≠ch {cumulative_variance[n_components_final_pca-1]*100:.2f}% ph∆∞∆°ng sai, c√≥ th·ªÉ c·∫ßn th√™m PC."
                )
        else:
            print(
                f"Ch·ªâ c√≥ {len(cumulative_variance)} th√†nh ph·∫ßn ch√≠nh. T·ªïng ph∆∞∆°ng sai gi·∫£i th√≠ch l√† {cumulative_variance[-1]*100:.2f}%."
            )
        #### ======= END B·ªî SUNG KI·ªÇM TRA & BI·ªÇU ƒê·ªí 5.1
        ##===== Optimize Start===
        ##5.2. Ki·ªÉm tra overfitting c·ªßa PCA (Thay v√¨ #8 Ki·ªÉm tra ·ªü cu·ªëi c√πng
        train_size = int(len(X_cleaned) * 0.8)
        X_train_cleaned = X_cleaned[:train_size]
        X_test_cleaned = X_cleaned[train_size:]
        pca_train_final = PCA(n_components=n_components_final_pca)
        X_train_pca_final = pca_train_final.fit_transform(X_train_cleaned)
        X_test_pca_final = pca_train_final.transform(X_test_cleaned)

        X_train_reconstructed_final = pca_train_final.inverse_transform(X_train_pca_final)
        X_test_reconstructed_final = pca_train_final.inverse_transform(X_test_pca_final)

        mse_train_final = mean_squared_error(X_train_cleaned, X_train_reconstructed_final)
        mse_test_final = mean_squared_error(X_test_cleaned, X_test_reconstructed_final)
        print(f"\nMSE tr√™n t·∫≠p train (PCA): {mse_train_final:.4f}")
        print(f"MSE tr√™n t·∫≠p test (PCA): {mse_test_final:.4f}")
        print(f"Ch√™nh l·ªách MSE (PCA, c√†ng nh·ªè c√†ng t·ªët): {abs(mse_train_final - mse_test_final):.4f}")
        ##===== Optimize end===

        # 5.3. Ki·ªÉm tra t√≠nh d·ª´ng b·∫±ng ADF
        # Check stationarity for 'VNINDEX' and the generated PCs
        vars_to_check = ["VNINDEX"] + [f"PC{i+1}" for i in range(X_pca_df.shape[1])]  # Check all generated PCs
        adf_results = {}
        needs_differencing = False  # Flag to see if differencing is needed

        for var in vars_to_check:
            data_to_check = df_cleaned["VNINDEX"] if var == "VNINDEX" else X_pca_df[var]
            if not data_to_check.empty:
                try:
                    adf_result = adfuller(data_to_check)
                    print(f"\nKi·ªÉm tra ADF cho {var}:")
                    print(f"ADF Statistic: {adf_result[0]:.4f}")
                    print(f"p-value: {adf_result[1]:.4f}")
                    adf_results[var] = adf_result
                    if adf_result[1] < 0.05:
                        print(f"{var} l√† chu·ªói d·ª´ng (p-value < 0.05)")
                    else:
                        print(f"{var} kh√¥ng d·ª´ng (p-value >= 0.05), c·∫ßn sai ph√¢n.")
                        if var == "VNINDEX":  # Only difference the target if needed
                            needs_differencing = True

                except ValueError as e:
                    print(f"\nError running ADF test for {var}: {e}. Skipping.")

            else:
                print(f"\nData for {var} is empty. Cannot run ADF test.")

        # If VNINDEX is not stationary, difference both VNINDEX and X_pca_df
        if needs_differencing:
            print("\nVNINDEX kh√¥ng d·ª´ng, ti·∫øn h√†nh sai ph√¢n d·ªØ li·ªáu.")
            y_diff = y.diff().dropna()
            X_pca_df_diff = X_pca_df.diff().dropna()
            y_final = y_diff
            X_pca_df_final = X_pca_df_diff.loc[y_final.index]  # Align indices after differencing
        else:
            print("\nVNINDEX ƒë√£ d·ª´ng, kh√¥ng c·∫ßn sai ph√¢n.")
            y_final = y
            X_pca_df_final = X_pca_df.copy()  # Use original PCs if no differencing needed

        ## B·ªï sung ki·ªÉm ƒë·ªãnh sai ph√¢n ƒë·ªÉ x√°c ƒë·ªãnh t√≠nh m√†u v·ª•
        # Ki·ªÉm tra t√≠nh d·ª´ng sau sai ph√¢n
        print("\nKi·ªÉm tra ADF sau sai ph√¢n:")
        adf_result_y_diff = adfuller(y_final)
        print(f"ADF Statistic (y_diff): {adf_result_y_diff[0]:.4f}, p-value: {adf_result_y_diff[1]:.4f}")
        for i in range(X_pca_df_final.shape[1]):
            adf_result_pc = adfuller(X_pca_df_final[f"PC{i+1}"])
            print(f"ADF Statistic (PC{i+1}): {adf_result_pc[0]:.4f}, p-value: {adf_result_pc[1]:.4f}")

        ## ======= B·ªï sung Chart ki·ªÉm ƒë·ªãnh ADF
        # Ensure X_pca_df_final is not empty and has columns befor building ARIMAX
        if not X_pca_df_final.empty and X_pca_df_final.shape[1] > 0:
            # 6. X√¢y d·ª±ng m√¥ h√¨nh ARIMAX
            # Check if y_final and X_pca_df_final have the same index length
            if len(y_final) == len(X_pca_df_final):
                train_size = int(len(y_final) * 0.8)
                y_train = y_final[:train_size]
                y_test = y_final[train_size:]
                X_train_pca = X_pca_df_final[:train_size]
                X_test_pca = X_pca_df_final[train_size:]

                print(f"\nShape of y_train: {y_train.shape}")
                print(f"Shape of y_test: {y_test.shape}")
                print(f"Shape of X_train_pca: {X_train_pca.shape}")
                print(f"Shape of X_test_pca: {X_test_pca.shape}")

                ## ==== C·ªë ƒë·ªãnh SARIAX(p, d, q) = (1, 0/1, 1)
                # Determine the 'd' order for SARIMAX. If differenced, d=0. If not differenced and originally stationary, d=0. If not differenced and not stationary, d=1 (should be handled by differencing step).
                # Assuming the differencing step handles non-stationarity
                sarimax_p_order =0
                sarimax_d_order =1
                sarimax_q_order =1

                # Check if train data is sufficient
                if (len(y_train) > 0 and len(X_train_pca) > 0 and len(y_train) == len(X_train_pca)):
                    # Hu·∫•n luy·ªán #1 m√¥ h√¨nh ARIMAX (p=1, d=0 , q=1). Use d=0 if differenced manually.
                    # Hu·∫•n luy·ªán #2 m√¥ h√¨nh ARIMAX (p=1, 1, q=1).
                    # Hu·∫•n luy·ªán #3 m√¥ h√¨nh ARIMAX(1, 1, 0)
                    try:
                        model = SARIMAX(
                            y_train,
                            exog=X_train_pca,
                            order=(sarimax_p_order, sarimax_d_order, sarimax_q_order), ## change 0 => cho hu·∫•n luy·ªán #3
                            enforce_stationarity=False,
                            enforce_invertibility=True,
                        )  # Added enforce=False for robustness
                        arimax_model = model.fit(disp=False)
                        print("\nT√≥m t·∫Øt m√¥ h√¨nh ARIMAX:")
                        print(arimax_model.summary())

                        # D·ª± b√°o
                        if (len(y_test) > 0 and len(X_test_pca) > 0 and len(y_test) == len(X_test_pca)):
                            y_pred = arimax_model.forecast(steps=len(y_test), exog=X_test_pca)
                            # Ensure y_test and y_pred are aligned before calculating RMSE
                            if len(y_test) == len(y_pred):
                                rmse = np.sqrt(mean_squared_error(y_test, y_pred))
                                print(f"\nRMSE tr√™n t·∫≠p test: {rmse:.4f}")
                            else:
                                print("\nPrediction length does not match test data length. Cannot calculate RMSE.")
                        else:
                            print("\nTest data (y_test or X_test_pca) is empty or mismatched length. Skipping forecasting.")
                    except Exception as e:
                        print(f"\nError fitting or forecasting ARIMAX model: {e}")
                        print("Check your ARIMAX order (p,d,q) and the stationarity")

                #print(f"\nTham s·ªë ARIMAX t·ªëi ∆∞u (p, d, q): {best_pdq} v·ªõi AIC: {best_aic}")
                ##--- end t√¨m tham so toi ∆∞u
                # Ki·ªÉm tra m√πa v·ª• tr√™n y_diff
                plt.figure(figsize=(12, 6))
                plot_acf(y_diff, lags=252)
                plt.title("ACF of VNINDEX after Differencing")
                plt.show()
                plt.figure(figsize=(12, 6))
                plot_pacf(y_diff, lags=252)
                plt.title("PACF of VNINDEX after Differencing")
                plt.show()

                # --- added Ki·ªÉm tra StartDate/Endate Report sample
                print(f"\nStart date of training data: {y_train.index.min()}")
                print(f"End date of training data: {y_train.index.max()}")
                try:
                    model = SARIMAX(
                        y_train,
                        exog=X_train_pca,
                        order=(sarimax_p_order, sarimax_d_order, sarimax_q_order),
                        enforce_stationarity=True,
                        enforce_invertibility=True
                    )
                    arimax_model = model.fit(disp=False)
                    print("\nT√≥m t·∫Øt m√¥ h√¨nh ARIMAX:")
                    print(arimax_model.summary())
                    print(f"\nRMSE tr√™n t·∫≠p train: {np.sqrt(mean_squared_error(y_train, arimax_model.fittedvalues)):.4f}")

                    # D·ª± b√°o
                    if len(y_test) > 0 and len(X_test_pca) > 0 and len(y_test) == len(X_test_pca):
                        y_pred = arimax_model.forecast(steps=len(y_test), exog=X_test_pca)
                        # Ensure y_test and y_pred are aligned before calculating RMSE
                        if len(y_test) == len(y_pred):
                            rmse = np.sqrt(mean_squared_error(y_test, y_pred))
                            print(f"\nRMSE tr√™n t·∫≠p test: {rmse:.4f}")
                        else:
                            print("\nPrediction length does not match test data length. Cannot calculate RMSE.")
                    else:
                        print(
                            "\nTest data (y_test or X_test_pca) is empty or mismatched length. Skipping forecasting."
                        )
                except Exception as e:
                    print(f"\nError fitting or forecasting ARIMAX model: {e}")
                    print("Check your ARIMAX order (p,d,q) and the stationarity")
##=============== Optimize chuy·ªÉn ki·ªÉm tra Overfitting l√™n tr∆∞·ªõc khi quy·∫øt ƒë·ªãnh gi·∫£m chi·ªÅu =====

# 7. D·ª± b√°o cho c√°c ng√†y trong t∆∞∆°ng lai (2017-05-29, 2017-05-30, 2017-05-31)
data = pd.read_csv("VNINDEX.csv", index_col="Day", parse_dates=True)
y_full = data['VNINDEX']
future_dates = pd.to_datetime(['2017-05-29', '2017-05-30', '2017-05-31'])
X_future_raw = data.loc[data.index.isin(future_dates)]  # Gi·ªØ nguy√™n t·∫•t c·∫£ c·ªôt, bao g·ªìm VNINDEX
y_future_actual = y_full.loc[y_full.index.isin(future_dates)]

# Ki·ªÉm tra s·ªë h√†ng c·ªßa X_future_raw
print(f"\nS·ªë h√†ng c·ªßa X_future_raw: {X_future_raw.shape[0]} (n√™n l√† 3)")
if X_future_raw.shape[0] != 3:
    print("C·∫£nh b√°o: S·ªë h√†ng c·ªßa X_future_raw kh√¥ng kh·ªõp v·ªõi 3 ng√†y d·ª± b√°o.")
    print("Ch·ªâ s·ªë c·ªßa X_future_raw:", X_future_raw.index.tolist())

# Chu·∫©n h√≥a d·ªØ li·ªáu 3 ng√†y b·∫±ng c√πng scaler
X_future_scaled = scaler.transform(X_future_raw)
X_future_scaled_df = pd.DataFrame(X_future_scaled, columns=X_future_raw.columns, index=X_future_raw.index)

# Lo·∫°i b·ªè c·ªôt VNINDEX sau khi chu·∫©n h√≥a
if 'VNINDEX' in X_future_scaled_df.columns:
    X_future_scaled_df = X_future_scaled_df.drop(columns=['VNINDEX'])

# Apply the same cleaning steps to the future data
if 'noisy_variables' in locals() and noisy_variables:
    cols_to_drop_noise_future = [col for col in noisy_variables if col in X_future_scaled_df.columns]
    X_future_cleaned_noise = X_future_scaled_df.drop(columns=cols_to_drop_noise_future)
    print(f"Removed noisy variables from future data: {cols_to_drop_noise_future}")
else:
    X_future_cleaned_noise = X_future_scaled_df.copy()

if 'variables_to_drop_redundant' in locals() and variables_to_drop_redundant:
    cols_to_drop_redundant_future = [col for col in variables_to_drop_redundant if col in X_future_cleaned_noise.columns]
    X_future_cleaned = X_future_cleaned_noise.drop(columns=cols_to_drop_redundant_future)
    print(f"Removed redundant variables from future data: {cols_to_drop_redundant_future}")
else:
    X_future_cleaned = X_future_cleaned_noise.copy()

# Ki·ªÉm tra kh·ªõp c·ªôt v·ªõi X_cleaned
if set(X_future_cleaned.columns) != set(X_cleaned.columns):
    missing_cols = set(X_cleaned.columns) - set(X_future_cleaned.columns)
    extra_cols = set(X_future_cleaned.columns) - set(X_cleaned.columns)
    print(f"C·∫£nh b√°o: C√°c c·ªôt kh√¥ng kh·ªõp gi·ªØa X_future_cleaned v√† X_cleaned!")
    print(f"Thi·∫øu c·ªôt trong X_future_cleaned: {missing_cols}")
    print(f"C·ªôt th·ª´a trong X_future_cleaned: {extra_cols}")
    raise ValueError("Feature mismatch between X_future_cleaned and X_cleaned.")

# Ki·ªÉm tra s·ªë h√†ng c·ªßa X_future_cleaned
print(f"S·ªë h√†ng c·ªßa X_future_cleaned: {X_future_cleaned.shape[0]} (n√™n l√† 3)")
if X_future_cleaned.shape[0] != 3:
    print("C·∫£nh b√°o: S·ªë h√†ng c·ªßa X_future_cleaned kh√¥ng kh·ªõp v·ªõi 3 ng√†y d·ª± b√°o.")
    print("Ch·ªâ s·ªë c·ªßa X_future_cleaned:", X_future_cleaned.index.tolist())

# Transform the cleaned future data using pca_final
X_future_pca = pca_final.transform(X_future_cleaned)
X_future_pca_df = pd.DataFrame(
    X_future_pca, columns=[f"PC{i+1}" for i in range(n_components_final_pca)], index=future_dates
)

# Ki·ªÉm tra k√≠ch th∆∞·ªõc c·ªßa X_future_pca_df
print(f"K√≠ch th∆∞·ªõc c·ªßa X_future_pca_df: {X_future_pca_df.shape} (n√™n l√† 3 h√†ng, {n_components_final_pca} c·ªôt)")
print("Ch·ªâ s·ªë c·ªßa X_future_pca_df:", X_future_pca_df.index.tolist())

# D·ª± b√°o
try:
    if 'arimax_model' in locals() and arimax_model is not None:
        plt.figure(figsize=(12, 6))
        future_forecast_diff= arimax_model.forecast(steps=len(future_dates), exog=X_future_pca_df)

        # Kh√¥i ph·ª•c gi√° tr·ªã g·ªëc n·∫øu ƒë√£ sai ph√¢n
        if needs_differencing:
            last_value = y_full[y_full.index < future_dates[0]].iloc[-1]
            future_forecast_undiff = [last_value] + [last_value + future_forecast_diff[:i].sum() for i in range(1, len(future_forecast_diff)+1)]
            future_forecast = pd.Series(future_forecast_undiff[1:], index=future_dates)
        else:
            future_forecast = future_forecast_diff

        future_results = pd.DataFrame({
            'Date': future_dates,
            'Actual_VNINDEX': y_future_actual,
            'Predicted_VNINDEX': future_forecast,
            'Error': abs(y_future_actual - future_forecast)
        })
        print("\nD·ª± b√°o, gi√° tr·ªã th·ª±c t·∫ø v√† sai s·ªë VNINDEX cho c√°c ng√†y 2017-05-29, 2017-05-30, 2017-05-31:")
        print(future_results.to_string(index=False))
    else:
        print("\nL·ªói: M√¥ h√¨nh ARIMAX ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán. Ki·ªÉm tra l·∫°i b∆∞·ªõc hu·∫•n luy·ªán m√¥ h√¨nh.")
        raise ValueError("ARIMAX model not found or not fitted.")
except Exception as e:
    print(f"\nL·ªói khi d·ª± b√°o: {e}")
    print("Ki·ªÉm tra m√¥ h√¨nh ARIMAX ho·∫∑c d·ªØ li·ªáu exog.")

# V·∫Ω bi·ªÉu ƒë·ªì thu g·ªçn ch·ªâ nƒÉm 2017
try:
    if 'arimax_model' in locals() and arimax_model is not None:
        plt.figure(figsize=(12, 6))

        # L·ªçc d·ªØ li·ªáu VNINDEX th·ª±c t·∫ø cho nƒÉm 2017
        y_2017 = y_full[y_full.index.year == 2017]
        plt.plot(y_2017.index, y_2017, label='VNINDEX Th·ª±c t·∫ø (2017)', color='blue', alpha=0.7, linewidth=2)

        # V·∫Ω c√°c ƒëi·ªÉm d·ª± ƒëo√°n
        if 'future_forecast' in locals() and 'y_future_actual' in locals():
            # N·∫øu ƒë√£ sai ph√¢n, s·ª≠ d·ª•ng future_forecast_undiff (gi√° tr·ªã kh√¥i ph·ª•c)
            if needs_differencing:
                last_value = y_full[y_full.index < future_dates[0]].iloc[-1]
                future_forecast_undiff = [last_value] + [last_value + future_forecast_diff[:i].sum() for i in range(1, len(future_forecast_diff)+1)]
                forecast_values = future_forecast_undiff[1:]  # B·ªè gi√° tr·ªã ƒë·∫ßu ti√™n (last_value)
            else:
                forecast_values = future_forecast

            # V·∫Ω c√°c ƒëi·ªÉm d·ª± ƒëo√°n
            plt.scatter(future_dates, forecast_values, label='D·ª± b√°o VNINDEX', color='red', marker='o', s=100, zorder=5)
            # V·∫Ω gi√° tr·ªã th·ª±c t·∫ø cho 3 ng√†y ƒë·ªÉ so s√°nh
            plt.scatter(future_dates, y_future_actual, label='VNINDEX Th·ª±c t·∫ø (D·ª± b√°o)', color='green', marker='x', s=100, zorder=5)

            # Th√™m nh√£n gi√° tr·ªã cho c√°c ƒëi·ªÉm d·ª± ƒëo√°n
            for i, (date, pred, actual) in enumerate(zip(future_dates, forecast_values, y_future_actual)):
                plt.text(date, pred, f'{pred:.2f}', fontsize=9, ha='right', va='bottom', color='red')
                plt.text(date, actual, f'{actual:.2f}', fontsize=9, ha='left', va='top', color='green')

        plt.xlabel('Ng√†y', fontsize=12)
        plt.ylabel('VNINDEX', fontsize=12)
        plt.title('D·ª± b√°o VNINDEX nƒÉm 2017 v√† K·∫øt qu·∫£ D·ª± ƒëo√°n (2017-05-29 ƒë·∫øn 2017-05-31)', fontsize=14)
        plt.legend(fontsize=10)
        plt.grid(True, linestyle='--', alpha=0.5)
        plt.xticks(rotation=45)
        plt.gcf().autofmt_xdate()
        plt.tight_layout()

        # L∆∞u bi·ªÉu ƒë·ªì
        plt.savefig("VNINDEX_forecast_2017.png", dpi=300)
        plt.show()
        plt.close()

        try:
            files.download("VNINDEX_forecast_2017.png")
            print("\nƒê√£ l∆∞u bi·ªÉu ƒë·ªì d·ª± b√°o 2017 v√†o 'VNINDEX_forecast_2017.png'")
        except NameError:
            print("\nCh·∫°y trong m√¥i tr∆∞·ªùng kh√¥ng ph·∫£i Colab, b·ªè qua t·∫£i file.")
            print("Bi·ªÉu ƒë·ªì ƒë√£ ƒë∆∞·ª£c l∆∞u c·ª•c b·ªô: VNINDEX_forecast_2017.png")
    else:
        print("\nL·ªói: M√¥ h√¨nh ARIMAX ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán. Kh√¥ng th·ªÉ v·∫Ω bi·ªÉu ƒë·ªì d·ª± b√°o.")
except Exception as e:
    print(f"\nL·ªói khi t·∫°o bi·ªÉu ƒë·ªì d·ª± b√°o: {e}")
    print("Ki·ªÉm tra d·ªØ li·ªáu ƒë·∫ßu v√†o (y_full, future_forecast, y_future_actual) ho·∫∑c c·∫•u h√¨nh matplotlib.")
